 Okay, so I'm going to describe a verbal interface. This is the background of what it is as a total mission that I want to develop, and then we'll describe the individual pieces in greater detail, and I will have you build them out. I also have a sort of WIP or work in progress implementation of this that I will give as reference but let's start fairly significantly from scratch and worry about using that later


 So the work-in-progress implementation was intended to use a custom ORM. I want to avoid doing this because that thing is a living hell and is giving me massive, massive problems. And I also potentially at least want to avoid doing this in Python because I'm tired of the speed constraints that that thing has. So, I don't know exactly what language I want to build this in. I'm just going to give you this code for reference.


 Ultimately, the mission is that there be a stream of words coming from a transcription system. I've built one implementation of this using a voice activity detector and then just peeling off segments of speech as the voice is no longer present for such and such period of time. and then transcribing that and giving a total transcription and then creating from that a linked list of words. But I want to put all of that aside and I'm going to have you, first of all, code up a sort of abstract class that is simply a live word stream. And its only goal will be to, it will have a start and stop listening. and then a iterator that regardless of whether or not it is listening the iterator will simply peel off words so when you call start or stop listening that may be done in another thread and so it will need to be thread safe in the future but you can start with simply the abstract interface start listening, stop listening and an iterator for so called words or as they're shown in this example to be word nodes you can just call it word and it will be a linked list node that is basically it won't be specifically a linked list node but it will follow the same kind of pattern as a linked list node it will return to you an iteration of nodes it will just yield out nodes or words rather so you can imagine that under the hood the transcription service may be producing words faster than is being processed. And so those will be appended to you know by reference They be setting the next node on the word to whatever the next word is and so even if the iterator hasn gotten to that yet you're just gonna have this individual word that will have reference to the next word that may become populated at some point by another thread or some other process and then the previous word will be iteratable all the way back to the point where the application first started. That word node, unlike what is shown in the example, will not just be a timestamp. It will be a start time and an end time, which in Python would be a date time object but in anything else it it would be something similar. Okay? You can have a string function if you want, but that so-called word will presumably be any sort of actual, like... I'm only going to use English. I'm only going to think about English right now. It will be any sort of word, English word, without any form of punctuation around it or any sort of spacing around it, or it'll be a number. and that's it. Very simple. And so this iterator will sit there iterating over all of the words and just yielding them out. Now, who consumes that is... I have an example of it in app.py. basically and this isn't this is not the full implementation but it will whoever consumes it the so called event controller let's call it let's define a class that is the event controller or application. I don't know exactly what to make it just yet, but its job will be to, given whatever screen is currently open or whatever other controls are listening for words, it will pass those words down to the controls to validate whether the control wants to decide whether or not to use it So it works very similar to an event system in normal graphic user interfaces whereby each control that is on a screen will be given the current word and asked to validate it whether or not basically find out whether or not it wants to use the event and if it does use the event then it will return true or some enum I haven't figured out if there will be any other types of returns, but it will inform the event controller that it has used the event, and the event controller's job then is to not send it to any other controls that are in the screen's list of controls. So each screen will have a list of controls, and the event controller will iterate through the current screen's controls for every single word. It will tell every word or it will tell every control within the screen that a new word has been received every time there is a word up to the control that returns that it has used the word. So it will iterate the screen, the controls in the screen rather and it will say in order to each of those you got a new word and the first one that returns that it has used the word will return true and at that point it will no longer iterate any of the controls on on that screen until it gets the next word. So it's for word in words from this, or for word in, you know, WordStream, live WordStream, and then control in screen controls, and then control validate, and if validate, you continue, or not continue, you break. right then there is in the event controller and this is not implemented in the example there should be the ability of controls to also receive global knowledge that there was a word regardless of whether or not something uses the event of there being a word received or passed down even if a control uses that word then that list of controls that is in the like global awareness can receive the word and just do something with it it's not returning anything when it does something with it it is just consuming every word that's produced and so it's kind of like a signal actually you just get an event that you can register for that you know if you want to you can listen to all words that are produced and so the event controller holds on to a word stream and the word stream has the ability to start and stop listening that does not break the iterator the iterator will continue even if listening was stopped. That is at least the protocol that will be used when such iterator is implemented. And then the event processor will hold a so-called current screen and actually the event processor is sounding more like an application instead of the event processor being event processor we could also call it application so the application would have a current screen and it would pass the word to each one of the controls or to the event that a new word is received and so that's just a summary that's that's the application and generally speaking what a control is and what the application is and so the code that I'm going to give you is going to contain all sorts of other stuff for web app crap shit, don't pay too much attention to that just yet.


 A control is very simple in its just or in its basic form. it will have some text associated with it that will be shown on whatever button is displayed on an actual graphic user interface I don't know that it really needs an ID that was just added by AI it will have a list of key phrases which can be used to validate words from the application slash event stream whatever you want to call it and what it will do is it will upon initialization currently it's doing it every time check phrase is called that's not ideal so upon initialization it will break down each one of those key phrases into a list of words which the function to do that will exist on the word iterator the live word iterator the thing that has start listening stop listening and words which will be an iterator that class that does that, or that abstract class that does that, we'll have another abstract method as I'm thinking about this that will be take some long string and break it down into a list of words. And those words that it will break it down into will be a... or a list of word nodes And all of those word nodes will be linked together Their previous and next references will be set accordingly. and you'll just put them in a list for convenience of access so that a method like validate word inside of a control can use it to get the last word of a given phrase every time it wants to check and see if a given word that it is validating matches one of the key phrases. And what its job is, as you can see in the example code provided, is that it will take a given word that it has passed to the validate function and it will just compare it to first the last word of each key phrase, every one of the key phrases. And if it matches one, then it will iterate back from there, checking to see if it matches the rest of the words of the key phrase. So basically, the validate function's job is to wait for for a given keyphrase's last word. And then it will iterate each of the keyphrases. And for each one of the keyphrases, it will iterate backwards from the last word in that keyphrase and check to see if the entire keyphrase matches word by word. and if it does match then it will return true and it no longer needs to validate any of the other key phrases or rather I say it will return true I don't know if there needs to be other return types it will return indication that it has used the event at which point it will at least for now and then the function will call its action function So each control will have an action or rather a callable that takes no input and at least for the moment returns nothing We can improve that maybe in the future but that callable being a separate little object that it can hold a reference to will also be nice to facilitate later that the GUI be able to call the controls action if something like a physical GUI button is pressed or some such thing. The example implementation validate word inside the control of the example code I'm going to give you is pretty good. We just need to deal with the new interface that we're going to write here in whatever language we choose to write. That control, when it's validating words, you'll notice also needs to use a number map to sort of replace the current word with anything that's in the map. So you have to iterate through all of, you know, like if it's in the map, then you have to iterate not just, or you have to not just compare the current word that you're validating to whatever the current word is and whichever of the key phrases it is that you're iterating, you have to compare the mapped version of it to all of the things it maps to. The example, again, shows this pretty decently. It's just to handle things like numbers. we might expand that map in the future to instead of simply being a map of numbers like the word zero to the number zero and the number zero to the number zero things like that instead of just doing that we could do you know like the word zero or the number zero both go to a list of words and so or a list of strings rather that way uh we can perform a comparison you know like if uh whisper tends to transcribe things um differently different times then we can expand that map in the future to include the different ways that it likes to spell things or say things or whatever. Ideally it would be nice to just retrain Whisper but for right now I'd like to have that map in the background just so that I can handle its little gotchas. So I guess the iteration would actually be when you're validating a word you would go through each one of the key phrases you would start with the last word of that given key phrase and you would compare it iterating back through the rest of the words in the key phrase comparing it as you go and your comparison between the word that you're currently validating against in your key phrase you know like you have key phrase word and you have validate word or word under test. You would just grab the string of the word under test and look it up in the map. And if it exists in the map, then you would compare all of those words to the current word of the key phrase that you're on. That way, you know, you can expand a map of words that are spoken and then transcribed differently at different times. And the initial map that we could have is just, you know, the word zero goes to either the number zero or the word zero and you know so on and if it's not in the map then obviously your validate function would just compare the current word under test and the current word of the key phrase that you're on


 Okay, so moving on. There is then a so-called screen stack. The purpose of this is simply navigation. is to remember which screen you were on and when you were on it. The implementation that we have here looks to be good. And so the application can actually just hold on to one of this because this has a current screen. That's actually much better. So the application can just hold on to an instance of one of these so-called screen stacks, and that will be what it uses for navigation. The so-called screen stack can also have a forward button and a back button, however, or a forward method and a back method, I guess. And that would... Anytime you go back, you... This current screen stack is not fully implemented the way I would want it to in order for a forward button to work. But having the back button is easy. The back button is basically... or the back function is just basically pop. So that already exists. But if you wanted to have a forward method, that would be good And the way that would work is you call forward and it would return true or false whether or not it had actually changed the screen and then whoever's using the screen stack namely the application or whichever control is calling forward would be able to then get the current screen from the screen stack if, you know, it had returned that it had gone forward. And the screen stack's job would be that every time push is called, you just append to a list or not pop, not push, sorry, not push. every time pop is called you push the current screen onto a list of previously on screens so long as the list of the stack is greater than one you push it onto or you append to that list rather and then and anytime you call push you just clear that list of screens because you no longer have the ability to go forward you're now on some completely new screen your original uh like remembering of what screens you could jump back forward to um as is no longer valid so you clear in the push function and then in the pop function you can simply append to that list and then your forward function is you peel off the end of that list so last in first out and you just reduce the length of the list as you pulling from it I guess you would be popping from the end of the list And you return true if there was something in the list And if there is not, you return false. And then whatever is doing the call to forward can just get the current screen after that. this would necessitate of course the need of a hidden push function that or an internal push function that doesn't uh clear the um forward method or the forward list because you know then it would end up you would go forward one screen and you would end up clearing out the uh forward list and you wouldn't be able to go forward to the next screen that you were on or the most previous screen you were on any longer. So the hidden push would have to be called by push and push would have to both call the hidden push and then also clear the forward screens or the forward list of screens. so we got a stream of words coming in we got our application that you know like consumes and sends those down to the current screen the current screen is maintained by a screen stack class there's a so-called screen which as you're taking it from the example code can be subclassed and include other other functionality than just what its base implementation is. And there controls So now let talk about the base implementation of the GUI or the graphic user interface that I want for a so-called screen. Right now we've been very abstract. We're keeping it to base classes. This is the core functionality that I want to be unit testable or very significantly unit testable. The GUI code will be probably less unit testable than this. So I want this portion to be very well built and well tested in the not too distant future. And this is where it'll be nice to be able to build out actual unit tests that have examples of a user interacting with the system. And such examples might be, you know, like a bunch of spoken text that, you know, is all saved, is all just in plain text saved with the unit test so that these components can iterate through that list of words and make sure that the right controls are called on the right screens and the unit test can define a set of screens, a set of controls, and that be the test of this whole system. so it can kind of take in the the place of that abstract speech-to-text system that we were talking about before in the unit test and just feed in text word by word from a text file in order to unit test this entire like base system for the user interface the core user interface framework


 And just a side note too, one of the nice things about this is because if that whole system was well built and well tested, then there could be other word streams coming in as well. like there could be word streams coming in from a large language model which would then give it sort of indirectly or very directly control over the entire user interface by spoken language that because the user is able to speak to it verbally they would be able to give example not directly fine-tune or train a large language model but they would be able to give an example, which could be used for fine-tuning in the future, but they would be able to talk to the system, use it to their heart's content, and use that as an example for a large language model to learn how to use it itself. So that's just kind of a side benefit. but moving on to this so-called screen I want there ultimately to be an interface where it would be nice initially if it was a web interface ultimately I like more local things like apps and I'm used to writing Qt Python applications or more local applications I'd like it to be that in the future but for right now going down this web app way of doing it is easier so let's describe visually what the default screen class should be. As you can see I want there to be the ability of screen to be subclassed and so they're not necessarily is always going to be the the same exact HTML file for every screen. But I want there to be reusability between components in the HTML so that other screens can be built on top of it. the core screen object or the core screen control or set of controls not to be confused with the control that I said earlier, the word control that I had said earlier, that's like the custom application framework that we building that its so control control when I say control here I mean more like a GUI control like on the web form like a button or some such thing right now we're just gonna start with buttons and and we're going to have a menu bar. The so-called menu bar will just be a HTML to and whatever associating JavaScript and CSS and all of the stuff necessary to make buttons appear on the screen that represent the so-called controls of the app or of the current screen. So whichever screen you're on, there's usually going to be a menu bar, and that menu bar will just have in it a list of buttons where if you were to click either or any of the buttons, each button is associated with a given control in the core framework. And that core framework is the whole unit tested, like well-built thing that I was talking about before. Now we're talking about the web side of things. So the web side of things, button equals control. when you click on a button it calls the core frameworks associated control action and that action is then performed on the server I would have to do that by some sort of remote procedure call like it has to actually call a method or a post or something, I don't know how it works I'm not a web developer, I'm mostly local software developer. So it has to call into the server and actually effectively call the action on the server. Any time however, and this is a requirement of buttons, anytime a button is clicked on the web form I want the button to animate in order to show that the button was clicked but I also want the button to animate whenever the core framework calls the controls action so when you're doing that validate word stuff that returns true the core application should fire an event at that point to any web view of the application and notify it that the given button on the given screen was clicked. And this, you know, might bring up a good point or a good thought where instead of the controls just being married together via like the name or we had this so-called ID string that I believe was manually set in the application code, a control probably should have a UUID so that it can be married between the core application and the web form. And that way, the web form can be notified that a control was activated via spoken word or notify the core application if the user clicked the physical button on the menu bar. And so that's the communication with buttons and how they tie into controls in the core framework. the job of the menu bar in the HTML side of things is simply to make all of the buttons displayable. They all have to fit on the screen I want the menu bar to be at the bottom of the screen and I want the menu bar to also display the last transcription the so-called last transcription. Now the way I want that to work is the core application should have an event that can be registered for that it can fire internally that informs anyone who's interested in it what the last so-called transcription was. And we'll worry about how to implement that later because it's going to marry very tightly to the word iterator, the live word iteration object, the abstract class that we're pulling all of this stuff from. but the general job of that function will simply be to notify something like the web UI in this case that there is some new transcription that has occurred from the user Maybe, you know, like initially, my implementation of the word iterator is going to be something very similar to what I already have, where it uses a voice activity detector under the hood to just monitor for when the user is speaking. and then when they stop speaking for so long it just goes ahead and transcribes the entire voice segment and then returns a transcription and turns that into a linked list of processed, filtered out words that are just English words and numbers or is all lowercase, all very easy to use in a UI like this or a voice user interface rather than a graphic user interface, but also any other kind of interface, right? And it'll just, the application will just have that event that will just peel off the actual entire transcription exactly how it was from Whisper or whatever transcription service I use. And the web form side of things or the graphic user interface side of things will simply listen to that event and it will display whatever the last transcription was on the bottom of the menu bar. The very bottom of the menu bar will simply say, last transcribed, in quotes, whatever it was that was transcribed. And, you know, maybe a future version of things, you know, the word iterator might just look for pauses between words. And if the pause is long enough, then it just peels off all words up to the point of the pause and displays that as the last transcription, concatenates a string and just returns that or throws an event for that. If I have a more word-level live transcription, it can do that instead. But your menu bar, that being a whole very large tangent, its only job is to display the controls in like a list initially or you know a grid or some thought such thing at the bottom of the screen or the bottom of the HTML for whatever other things the screen may do


 Now, moving on to the individual screens. There will be many. Initially, when the application is first launched, and actually you know eventually it will be nice if there is like sessions where you know different devices at least I'm not worried about accounts at the moment but you know eventually different user accounts will be able to log in or simply you know open up the home page for different sessions on different devices or the other one being user accounts you get the idea eventually it would be nice if there were sessions so the current page would be on a per device level it might be nice to implement that kind of logic up front when the website is first opened and it maybe sets a cookie or just, honestly, just do it based on the link. Don't even set a cookie. Just whenever the page is first loaded, that just creates a session. and that's that's the screen stack that that user gets or that session gets you know, it creates a new screen stack and just to clarify the job of the screen stack isn't to store like the list of screens that are like available by the entire application the job of the screen stack is simply to store the navigational history. So each session or each time the web browser is reloaded at the main home page of the web app, that creates a new session. And it's just tracked for so long. If it times out, then it times out and returns a timeout error. That would be nice. That would be really nice if the application kept track of a list of sessions whereby there were in each session a screen stack. but anyway whenever a session is opened or when the application first starts session management might be like a next revision maybe don't implement that in the application up front at least in the initial version let's maybe keep the initial application simple so that we can test all of this but you know when we implement sessions we can do all of that and then either way when a so-called session is started or when the application first starts depending on which version we're on of this thing the first screen that will open will be the so-called welcome screen and I think you get where this is going based on the example code I have you. When the user says, hey Alejandro, or any such similar combination of phrases, it will navigate to the main screen. And the main screen will have different screens on it or different controls on it as well, mostly for navigation. Every screen except for the welcome screen will have a control for cancel, which I really should rename, maybe back. I don't know why I called it cancel. That was stupid of me. But no, I was on the sidetrack. I was kind of parting from the norm of back just because you know like I was thinking of controls as like a verbal thing so you like you say that you want to do blah blah blah and blah and the application maybe misinterprets you and goes and starts doing something you can say cancel and it would it would take you back but let's just call it back because I don't think that that's going to happen very frequently so the so-called cancel or rather I'm going to call it back control will exist on every screen instead of or except for welcome and it will just perform the job of you know using the screen stacks pop method to go back sort of indirectly I guess it would I guess it would be calling into the application because the application needs to actually refresh or notify that the current screen had changed one could actually manage in the imagine in the future that the screen stack is kind of like the precursor to your session object so maybe it's the one that fires the event and just says hey session of this ID should you know and I'll navigate backward. But... Ultimately, I want there to be a conversation screen and a terminal screen at least initially. The conversation screen will be much like it is right now initially, where it just displays a list of conversations. And so little AI, you might be looking at these things in the future and you'll see the code that exists as we've continued to code together. Every time we close the session, I'm going to give you this description and you will see a lot of code in this description as part of your example for how to build code that we haven't, as I'm saying this, built yet. but in the future I'm going to modify this description a little bit so you might see some things like the model classes that we've been discussing in this example or you might see them already written just you give in reference to them and them already being built just to clarify because we might be in a different step of this along the way. So in the example implementation that I'm going to give you, that example implementation, as I'm thinking about this, may not contain an example of everything here that I'm describing. If it doesn't, then take it as to mean we've already built that part of it and I've removed it from the example just to save a little bit of context space but just be aware of that the conversation screen what it is intended to do and what's not very well implemented in the example it's not fully implemented it's not really fully working but it is close it does sort of work is that there will be a list of conversations and clicking on any of them will result in navigating to that conversation. A conversation has a UUID associated with it. in the code that I have here I've been calling that auto ID not ID just so that you're aware that comes from my ORM it's an artifact of how it builds IDs for objects automatically so I wanted the user to be able to include their own ID and not have it be a common string like ID or a common name like ID in case the user wanted to have their own the auto ID is just used by the ORM so be aware of that just in the examples but ultimately the goal of the conversation list is to simply move on to whichever conversation you want. Now, this is where, in my examples, I believe that the text that was on the button was used as one of the key phrases. I don want to do that for controls Because one of the controls that I want to have on that list of conversations that not currently implemented in the example that I going to give you or may give you if you are looking at this much later than I'm saying this one of the controls I want to implement its validate function will will be a little bit more complicated. It will expect of the user to first say the word, select conversation or the words select conversation. and then it enters this mode whereby it will use the event and see this is why this is why me talking through it is good because the example code that I'm going to give you the validation method for control simply returns true and and I had talked about doing it that way the entire time I've been talking here but this is where I finally see now that I do in fact need some multiple return values and I'd like to use an enum for any sort of multiple return values don't use integers or some dumb thing like a string or some shit but there's the so called use once and then there's hold on to or something along those lines or unused so unused would be similar to false use once would be similar to true how we were talking about things before where a control returns true if the word's used or whatever the special validation logic of this control is sort of a modal control and maybe it would be nice to be a subclass of control where a so-called modal control is like the one I'm talking about here with the conversation view or the conversation screen where you say, or conversation selection screen I guess where you say select conversation and that's the first key phrase whereby the control will be validating words as it's given to them or as it's given to it by the event system and when it sees the words select conversation now instead of returning use once it returns hold on to or use multiple times or whatever. And so what the event system will then do is it will... First of all, it should inform the UI, the GUI, the graphic user interface. It should inform it by whatever event it is using to animate that a button is clicked. it should inform it one of any number of different types of event using things where it can say that the control is holding on to the word stream instead of just that you should depress the button and make it look like it had been clicked verbally, now you can also make it look like it's being held visually or at least that's what the GUI will end up having to do with that information. But the event system will simply fire the event that a control has either been used or is being held. And so this conversation selection control, the first phrase would be select conversation. it would then return that it's holding onto things and the event system now every time a new word is received rather than iterating all of the controls of the current screen it is only ever going to inform that specific control that is holding onto it holding onto the event stream and the event that it's got to signal just for generally all controls to be aware that a new word is received. So if any control wants to receive a word like all the time, it can do that. But the main list of controls in a given screen will only be iterated if there is not a control in the screen that is now holding onto the list of, or the stream of words. And so the conversation selection thing you say select conversation that holds onto the word stream and now the user can just speak And they can speak whatever the hell they want to speak and then the moment they say end selection, that's when the control returns use once, or that it's done. maybe you don't need a different return value for that you just return use once and the event system interprets that you know once the control return that's that's holding on to the stream returns use once then it has released the the usage of the event stream and now anything else can process it from there and so now that control will fire presumably its action in this example it will select that conversation and it'll do that by any number of means maybe it'll just look through the list of conversations and compare word for word or you know maybe it does a more advanced comparison where it takes each name of every conversation and breaks it into a linked list of words using the applications method for that. Not the applications, the word streaming class thing, the live word transcription thing. The method in that for getting a linked list of words from a given string. Maybe it just uses that to perform the comparison and searches for conversations that way. That would probably be the most robust initially. Maybe eventually it hands it to a large language model and says, here's the list of conversations. Here's what it is that the user said that they wanted to select, figure out which one seems most probable and gets the answer from that. Any number of ways of doing this, right? but now presumably the select conversation action has everything it needs to select a conversation and the event system moves on with its life it is now listening to any words that come in and handing them down to all controls of the current screen like it was before and we move on this also allows us to implement other kinds of controls whereby you can do things like start speaking and stop speaking where you know the validate of a control for start speaking and stop speaking for like a long transcription control might take in the initial start speaking and then use that to hold on to the phrase or the event stream and then release once the user says stop speaking and maybe it looks for the time stamp it looks for how long ago did the user say stop speaking and how long before they said stop speaking did they say something else before that? So is the user continuing on with a phrase where they're talking about an AI that needs to stop speaking or are they saying the command stop speaking with a time both before and afterwards that that is more indicating to the AI or to the event system rather, to the controls validation function specifically, that it is actually intended to be addressed. And this is where the timestamps for the words are helpful to have, right? Anyway, moving on. that gives us sort of an additional form of control that needs to happen in the core application that additional type of control is like a modal control where it has a set of key phrases for the begin like maybe the begin key phrases and the end key phrases that'll be separate from each other and its validate function will be very similar in fact it would be nice if when you're implementing the control to make it so that the validate function is reusable or could be reusable by the modal control as well as sort of a hidden method public method kind of thing


 and so with the GUI or the graphic user interface I don't know how whisper transcribes it that's why I have to say orthographic user interface I don't know if it transcribed GUI the acronym correctly we need a conversation view as as well. And initially it will be very simple. Initially, you know, it'll be a modal control, start speaking and stop speaking being the key phrases. You don't need to look at timestamps for the modal control just yet because my transcription thing is probably not going to have accurate timestamps. So don't worry about that just yet. assume we'll have some timestamps there but don't use them for the modal control just to note I'll make the key phrases more complex in order to remove the ambiguity for now but the controls for the conversation screen once you've selected a conversation you navigate to a new screen that's the conversation screen before you had the conversations screen, the plural version. I might have misspoke. So you say, hey, Alejandro, that brings you to the main screen. The main screen's got conversations, terminal, and other things. The main screen, when you navigate to conversations, gives you a list of conversations. Once you get to the list of conversations, you can select a conversation. Once you select a conversation, you're on the conversation screen. the conversation screen, it will just be a back and forth. The formatting that I have for the conversation screen now is very nice. I like it. Try not to change it too much. Where you have the nice green background for one type of role and the nice red background for one type of role and the role is on the left and the message is on the right. It's very readable. It's very nice. I like it. Try not to change it too much. Maybe just regurgitate it into the actual files when we're ready to build that yet. We're not ready to build that just yet, but when you go and build that portion, go ahead and do that. it'll be a little confusing for you unfortunately because the examples that I'm giving you right here will all be in one file with all of these instructions and so it'll have all of the code for the original conversation screen but then at some future point you're going to be reading my general instructions here and non-condensed as though they are I don't want to go back over this again and summarize them so you may at that point see that there is conversation view code that's sitting off in the distance in its own files if you see that that means that you've already implemented it and you don't need to go regurgitating that code again just to be clear okay all right so the conversations or not the conversations the conversation screen will have the speak and stop speaking the job of that control will be to take the text that was said whatever was said between start speaking and stop speaking and it will simply paste it into to the text box for what to send, the new message. And if there was something already in the text box then it put a new line a couple of line breaks two new lines specifically between what was already in the text box and what it is that it's now pasting in and so this way you know the user can leisurely say start speaking to explain a whole bunch of stuff, stop speaking when they feel like it, and it'll just be sitting there ready to go for them to start speaking again if they so choose to and say a bunch more stuff. And they can just do that back and forth as much as they want. And then they can say, send. or send message now and it will send the message off to a large language model which would then respond back and stream the text into the view and you know then in the future maybe you can do other advanced things like a certain format you know if it if it writes code in a certain format and has a path both before and after the markdown code block then it saves it to actual file the application would actually save it to file and enable the chatbot to use it right but that's for a later date. There is also the terminal screen. So on the main screen, after you've said, hey Alejandro, you come to the main screen. The main screen has a terminal on it. If you say terminal, then it goes to the terminal screen. the terminal screen has an actual terminal window where there will need to be there is a terminal emulator class that I have and it has a get screen dump function and it has a send stream function there will need to be a send button or a send control rather and there will need to be a speak and stop speaking on the terminal screen as well that will allow the user to simply speak into the terminal window which isn't all too useful because you need to build a type special characters and other things that is difficult to say verbally and have the transcription service actually understand what it is that the user intended. And so they will have a dictate and end dictation as well. And the dictate function or the dictate control, once it is ended or once they have said end dictation, so you say dictation or or dictate and then you say things like open my git repository at such and such and dictation that text, open my github at such and such would be sent to a large language model with the sole purpose of asking it hey, the user said this what terminal command does that equate to or what should we run next in order to help them do that. Your response needs to be in a certain format, return it with both texts that you can say to the user and also what command it is that you wish to run And then you know like that would be broken up and displayed to the user like do you want to actually run this command yes or no which would be like you know maybe a terminal command screen which would simply its only job its special job is simply to display the text that the terminal or that the chatbot returned to the user like things that should be written or verbally said and maybe it verbally says it too but when you say shut up it stops speaking and when you say yes it goes ahead and pastes that into the terminal or the formatted version pastes that into the terminal and otherwise just disregards the entire conversation with the chatbot if the user says no or back or maybe if they go back it doesn't disregard it so that they can get to the terminal and then go forward again and be able to confirm so that's generally how the terminal works one thing that the example code does not show however is that I want the application and when we implement multiple sessions this is where the session object becomes a little bit more than just a screen stack. When we implement sessions, I want the session to have a list, or rather a dictionary of terminals as well, because there could be multiple terminals. And I want the conversation object to be able to, or the conversation screen, when you're chatting with a large language model eventually. And this will be the fairly immediate goal once we have all of that I have described so far working. And you can start with one terminal initially. But eventually, when we have sessions and we have multiple terminals, I want the conversation to be able to progress in a way where the user can say to a large language model that they're conversing with over a longer period of time, hey, I want you to write this application and describe it in the way that I have just described to you now and have it then open up a terminal window or any number of terminal windows by formatting its responses in a certain way, addressing bash blocks to terminals of specific names, and wherever a name or wherever a terminal of a specific name hasn't already been created for the particular session, then a new terminal with that name will be opened in that session and the chatbot will have its response parsed and directly sent to that terminal emulator. And it will be able to use things even like Vim. It will be able to send actual escape sequences and get a response back from the application without the user having to interact with it too much. That's the vision. That's the end goal or a end goal because one of the things I would also like the conversation to be able to have in the future, very near-ish future, is if you format your response in a certain way, it will take the request and send it to maybe a different language model, maybe start a new conversation, and that conversation will be the implementation of a new tool for the UI or a new control for this UI that I have so far described so that while on the go I can or anyone could interact with this thing and implement new controls for new screens while they're using the application and have the ai be writing new web page code from scratch while while the user is only interacting with the system by voice. so that's the entirety of the sort of of verbal user interface, verbal user or verbal slash graphic user interface that I wanted to implement here. This limits the usage of large language models where it can be limited to that of specific key phrases and facilitates the implementation of specific key phrases in an organized screen layout where the user can expand it automatically into the future. Or nearly, we would be able to nearly expand it automatically from the future. But this is the core system, the core graphics libraries that would be needed. I should state, things like the terminal screen or the conversation screen or the conversation list screen or the terminal action screen, anything and everything will have a menu bar at the bottom and that menu bar should be non-intrusive to any scrolling events that occur for the rest of the screen up at the top this is to make it very thumb accessible for a mobile device you can click any button on the menu bar and kind of thumb through the different screens but also the content like a conversation shouldn't be covered up by the menu bar the menu bar should require from whatever view is above enough space in order to not be ever covered up by the menu bar a way to properly implement this is to have you know one block above and one block below very frequently when we've tried to implement this in the past that ever went well we had to go back and forth many many times in order for that to work properly the example code that I have there I believe doesn't have this problem but I also had it a little bit more broken up I think where the last transcription text wasn't part of the so-called many bar or I don't know if it was or not. That'll be it. The main screen's HTML can simply have whatever the name of the screen is and then it has the menu bar at the bottom. But then things like conversation screen and internal screen and whatnot will implement their own screen code but they'll use a menu bar or they'll import a menu bar at the bottom. The CSS and whatnot should be organized. Whatever screens there are ideally there would be a folder in the new version for a given screen and it will have its own specific or screen specific css to go along with it inside that folder along with any custom javascripts that need to be run for that screen inside that folder uh let's keep things organized also uh the core stuff like the menu bar and whatever formatting it needs and some maybe you know more universal formatting rules uh can be outside in the main folder for the website that just being a little bit of organization that's the gist of it that's the whole thing


 now the next thing so we have the core portion we have the sort of web UI serving version and that all runs on this abstract interface for the live transcription that I've described and that has that method for the parsing of a string into a linked list of words, a list of linked list nodes of words, rather. Similar to how I described the model before, right? Not linked list node, following the pattern of a linked list node for a given word. Just to be clear, again, on that, because I don't want you to make a linked list node of a word. now what we're also going to have is a similar kind of thing for a chatbot I want to have a simple interface where all it is is either a complete transcription with a so-called weak or a weak reference you don't necessarily need to use weak references everywhere but just know that it will be a weak reference we're not going to be using it too directly back and forth but the transcription or not transcription there will be two objects that there can be either a like a completion stream or a completion streamed completion might be a better term for it completion and streamed completion completion. Now completion will, well each of them will have a reference to the large language model that made it. The instance rather of the large language model that made that particular completion or streamed completion. Streamed completion will have an iterate function that is simply an iterator of chunks. Chunk similar to a word, but not a word specifically, will have a previous and a next, just like with words from the transcription thing this interface sets the stage for something much more interesting which is requirements or or rather requirements about what is said or rather how it is said by a large language model. so these completions or streamed completion or chunks a chunk should like streamed completion and like completion they all should have the raw JSON JSON as originally created or what originally created them from the web request. And the stream completion should have a function for stop streaming. And it should have an iterator function of chunks. the stop streaming will presumably be thread safe so that anything anywhere in the application can tell it to stop streaming, tell it to shut up at any point that it wants But the iterator of chunk will have a given chunk will have a function on it to get the total completion up to that point. And it won't return a completion object directly, but it will just give you the string value. Just cache in it the string value. That's not something that would need to be serialized. It's just convenience so that the user can grab the whole thing up to that point. That's the basic interface. I don't want to get too far into the weeds for that. Next, later, I will describe how a large language model takes in a so-called conversation object. what a conversation object is, give examples for that along with a message and how I want to format the classes for that. But I need to get going, so I'll try to pick up for that later.



Okay, so the next part of this little Endeavor, 
I spoke of requirements. 
So, for each message, 
Generated by the AI. 
Or by the large language model. 
Uh, that 
Completion, or extreme completion is 
Like, 
Each of them end up, ultimately creating a message. 
And a message, you know, has Fields about it content. Which is just a string. 
Um, 
A day created. Various other things. 
And I'll get to how the model classes for. Messages and conversations. Later. But you can take a stub class right now. 
At least at this point. That is just a message. 
And it has a Content string and a date created. 
And you don't need anything else. 
So, 
This. 
Requirements. 
There will need to be a list of them. 
We'll worry about how the model classes work for this later. 
The way the requirement will work though is it'll take a message and it'll tell you whether or not the message passes, the requirement. Um, A simple type of requirement would be a large language model requirement. 
It actually passes. The. 
Message. Back to the large language model. 
An asset. Did you meet the requirement? 
But there could be other types of requirements as well. Uh, for instance. Did it? 
Or is it Json? Or is it XML or is it simply python and nothing else? Um, But let's stick with the Uh, large language model requirement first because that's the one that's most. The valuable. 
Moving forward. 
So, where this will work? Is you're gonna have a conversation? Uh, it's going to have a so-called 
Message sequence, or What is essentially the current list of messages. 
Uh, a conversation is actually a dag. So, Uh, each message has a previous message and children. 
Um, But, The. Conversation. You know whether it can Branch or not does have ultimately a list of messages that will be the current list that will be sent to a large language model in order to get that 
Streaming response or Um, 
Response. 
Or I guess I called it. 
I don't remember what I called it. I didn't call a response. 
But the thing from the large language model itself that which we generate of a response message from 
That gets added to the end of the conversation. Right. 
The. 
List of requirements. Is iterated for each new response from the large language model or would be right? So you have like, 
A function that you can pass a list of requirements. 
To, and also the 
Uh, current conversation or the list of messages. 
From the conversation, rather. 
Uh, The response from the large language model is 
And it checks to see. If? That response from the large language model is 
Meeting those requirements. 
And that function. You know, like I've described the 
The model classes for 
Going back and forth with the large language model. 
Uh, and the message and conversation object, a little bit. Um, but those are a little But fuzzy. Um, the main thing is that conversation. 
Or not the conversation. The 
Function for the requirements. 
And what it'll do. 
Is. 
Iterate through each one of the requirements. And basically, 
Up to this point in the conversation, with the large language model, the idea is that it will not have actually seen these requirements yet or maybe it will have in another form. Um, 
But, Optional. 
The large language model requirement. 
When evaluated you know, so you're going to have a list of requirements, you're going to check each one of the requirements against the chat Bots response or the last message in the conversation. And see if it meets that requirement. For a large language, model requirement though what you're going to do in order to do that? And this is the the one that I really want to focus the most on Um, You're? 
Have some description. Of what the requirement is. 
And, 
You know, how it's supposed to fulfill it? 
Normally. Like what would be a good response that would actually meet the requirement? 
And,
That text will be. Added onto the bottom of the conversation. Temporarily or not so much temporarily. But this is where you can see why it's nice to have a dag. Um, because 
Or not necessarily a full dag but you know, kind of 
Sparse tree because messages will also be removable in the future. But Anyway, don't don't Focus too much on the model classes, I keep falling back on them. 
When you? 
Ask. Did you meet this requirement? And you give it the the text for the requirement. 
At the end of the conversation. 
You. 
Get a response back from it. Telling you whether or not it met? The requirement. And if it did meet the requirement, or you get a response back from the large language model, right? 
If it tells you that it met, the requirement, you're good, you go on to the next requirement. And when you go on to the next requirement, you you Branch off of The message you're evaluating right? So you in this function, you were given a conversation 
Uh, up to the last message. Being from a large language model. Where you are? Uh, checking to see if it's responses are meeting a set of certain requirements. 
Uh, that last message in the conversation is called, like the perspective message, right? And so you're evaluating it against a list of requirements to see if it actually meets all of them. If it does, you're going to Simply return the conversation as it is, Um, now Including, I guess a bunch of requests to it of whether or not it met those requirements, but it'll be reset. Essentially to the point or the current list of messages in the conversation will be reset. To the point that it was when you first got it. 
So, 
You're gonna get this conversation with the perspective, message at the end, and you're going to be evaluating it against each one of these requirements. When 
Or requirement. 
Passes. You. Simply reset the conversation to what it was when you first got it. And you append. The next requirement to the end of the conversation and ask it if it met that one. And you repeat doing this over and over again until you hit a requirement where it does not pass. If you see one, where it does not pass, you're at sort of uh, interesting point. 
Um, 
You can ask it to fix it. 
Fix its response. 
The perspective message the perspective response that you were evaluating you could ask it to fix that response. To redo. That message given the requirement that you've now given it, or given the information in the requirement that you've now given it. 
But once you've asked it to meet that requirement, 
You're at kind of an interesting point, you can't ask it if it met the requirement now because from its perspective, it was just asked, did you do this thing? And then you tell it to do this thing when it says, no, it didn't. And so now if you ask it, did you do this thing? Uh you can't really get any useful information out of that. So you know presumably you could ask it to meet some. Formatting guidelines, for instance, of your code. Let's say you want it to use tabs instead of spaces. 
Um, If you tell it, you know, 
Did you use or if you ask it, did you use tabs instead of spaces for your code and it tells you no, you can then ask it. But can you make it so that it uses tabs instead of spaces? And this is kind of a contrived example, right? But you get the idea. 
Uh, if it's if it tells you here's the code with tabs. Uh, and you ask it now. Did you use tabs? Uh you're kind of in this point where it's it's gonna lie to you if it if it fucked up if it got into if it was maybe a really shitty large large language model. If it was a really small one. Uh, and it wrote the code again with spaces? Um, 
Then you know, it might lie to you and say, yeah, I I totally did what you told me to do even though it fucked up. Um, 
So, 
You can't reasonably. Expect it to. 
Be able to evaluate that requirement again. 
Except. 
And this is a concept I want to introduce here. 
If?
Because every time a requirement is evaluated, you're leaving it in the dag of the conversation, you know, it might not be the the act of message stream or the mass message sequence that's being used to generate the next response. Once this whole thing is done. But because you are leaving the request. Did you meet this requirement? No. Okay, here's how to meet it. Um, please meet it. 
Take your Take the new. Um, Output of that and put it. 
As an in, as a replacement of the perspective response, Because you're, you're leaving the The discovery or the the request of each requirement in the conversation at some point. 
Uh, you can actually 
Get feedback. From the user. 
The user can say. 
Uh, that was a good response or that, wasn't a good response. Like let's say, one of the requirements is do not apologize to the user under any conditions. You know, maybe I'm willing to go ahead and spend a few extra tokens. Uh, asking the server to revise. A message. Given to me by a large language model. 
Uh simply to get rid of its apologies to me, because I want to have my application speak verbally, using text-to-speech, whatever it is that. It's said to me and so I don't want it to say things like I'm sorry. Or. Sure, I can do that for you. Here's how to do it sentences. Like that are redundant useless worthless and a waste of tokens and a conversation and a waste of users time to sit there and listen to them. I don't want to listen to apologies ever under any conditions ever. So that might be a requirement that. I'm willing to go ahead and every single message to a large language model. I simply ask it. Did you apologize to me if you did remove it from your response and Give me a new draft here. A response. 
Um, 
So, 
If the user. Says that was a good response, you know, after it had gone through this redrafting process with the requirements 
You would have some information, some level of information, Uh, that presumably 
Uh, this requirements validation and Uh, or you know? Requirements, ensuring process. 
Had worked. And all of those requirements were met. 
So, 
Uh, because you have the data in the conversation. You can. 
Build out from the user feedback. If the user says that was a good response, if they give a thumbs up, You now know that every request to a Novet requirement was not met. Here's how to fix it. Resulted in it. Actually fixing it. And you have an example on how to meet requirements or how to 
Meet that particular requirement that you can use later. 
So because the user can leave a positive feedback or negative feedback. Even that informs you whether or not those requirements that were used to revise the message. 
You have a bunch of examples or congenerate. Uh, at that moment that you get the feedback examples of 
Good like requirement session. Requirement meeting session with a particular requirement. 
And bad ones. 
So, because a given requirement will have 
Positive and negative examples on how to meet it. 
That are attached to it and that goes for all requirements. So like a requirement will have a validate function. Uh, that an llm requirement would use a large language model in order to validate the requirement. Uh, but then you can have other kinds of requirements too. You know, like a format requirement. Um, which is check to see if it is valid, Json, if it parses and if it meets the particular schema, 
Um, 
Any requirement though, like they'll either of those just have different validate functions. 
Every requirement though is just going to use. Probably the original llm that the conversation is using. Um, you know, like so that validate function could be using a lower level. Large language model, but the 
Meeting the requirement could use the original one from the conversation. 
So, running through this whole thing for a moment, 
You have. The requirements. 
Pass to a function along with the conversation. Now.
Some large language models to add a little detail to this. Uh, or rather some apis for large language, models allow caching So, You would ideally cache. At this point. As well as. You know, like a cash can, uh, be bookmarked at several different places. You know, like so you can bookmark it Um, Bookmarking, it means that Uh, you're going to create. Uh, a cache of the hidden State of the large language model up to A certain point in the conversation. So the conversation Object has You know, like, 
A whole bunch of instructions. Maybe an entire code base in it, maybe a system prompt at the top, and then it'll say, from the user, write me some piece of code that does such and such Right? And then you're going to have the perspective message from the llm. 
That, that conversation. Is. With. 
And, You want to cash it at the point of the request, write me some code that does such and such and then you also want to bookmark it. Uh, after the perspective message. And the reason for this is, you know, you're going to be Iterating through these requirements. So, Uh, as long as you keep those bookmarks in place, 
You know, then every time you ask for a requirement, 
Uh hair did you follow this requirement? 
Here is some examples of it that were past and failed in the past. Um, 
It will tell you. Whether or not it met it. And then, You can ask it for a revision to the perspective message, if it did not meet the requirement, Um, Every time you ask it for a requirement because you've cached it up to at least the point of the perspective message. Then you. 
Uh, don't have to pay as much to the API or you don't have to run as much compute. On the 
Pre-Fill of the large language model up to that point. 
Similarly, and this is the reason for that other bookmark or the other cash bookmark. Uh, you have a bookmark up to the Of. Just before the perspective message. So that when a requirement is failed. You can also then 
Uh say please write me a revision for it. And then you can replace the perspective message. So the way that a replace what happen is, you know, the perspective message is still a child of the request. The original user request uh, that came before the perspective message. Um, It's still a child of it, but instead of it being the last message, 
The. Uh, message sequence for the conversation. It is now Uh, 
Not the last message. The new revision of the perspective. Message is the last message in the message sequence. And it's parent is also the original request. 
Um, But, 
And this is where. Things are nice. If you have Source information, if you have traceability in your model design, Um, 
Both. 
The perspective, message, the original request and the Uh, new revision have information about who created them. So, in the case of the original users request, maybe it's simply the user's name. Um, 
Then, in the Uh, case of Or maybe it's the the session, the login session with the application, which would contain the user's name along with, you know, date and time information about when the server was logged into when they actually were online in order to type messages, right? Um, 
But then, 
Uh, the perspective message would presumably have Source information about the Large language model, the settings that were used and which API call was used in order to generate that perspective message, 
Namely it might point to the completion object or the stream completion object that created it. 
Which points to the model? 
Settings about it. 
But this revised perspective message would also point. Uh, as its Source information to the perspective message or the revision that it is revising. 
And, uh, whichever requirement it is and whichever a large language, your model. It is that 
Did the revising? Given that requirement and which of the examples was used. In the revision. We're in the act of the revision. 
And so one of the nice things that happens here is, if you're only ever giving a subset of the examples, 
It's kind of, like, 
Uh, batches.
Where, you know, like if you're doing machine learning, Uh, you know, you're not necessarily looking at the entire data set all at once because you can't fit the entire data set in the GPU. It's kind of like that here. You can't fit all of the examples into the context of the large language model or maybe you don't want to pay to have it pre-fill that many examples into the hidden State of the large language model. Even if they could handle hundreds of millions of tokens, you don't want it to you don't want to pay for it, right? So you give it a subset of the examples for any given requirement each time that you evaluate the requirement and so maybe, you know, you only give it, uh, up to so many tokens. Maybe you give it so many examples, whatever your criteria are or you're stopping criteria are for when you have given it enough examples, Uh, for a given request. On both, you know, whether or not the requirement is 
Past or 
Asking for a revision. 
To a perspective message, given a requirement. And some set of examples, whichever one you're doing Each time you do something like that, you only give A random subset of the examples. And so now, because 
You have done a revision and you've replaced the perspective message, with this new revision to the perspective message, given some subset of examples. If you ask again, did you follow this requirement? But now you are giving a different subset or random subset of examples. Uh, you now Uh, are not doing that leading the witness thing anymore. Now you are essentially asking a slightly different example because we're a slightly different question. Because while the question might be worded the same while the text of the requirement, the description of the requirement might be the same, the examples, you're giving with that, requirement are no longer the same and therefore, you're effectively answering income in in total a Different. 
Uh, or you're asking in total a different question. So now, 
Um, 
You can ask that or you can ask if it meant that requirement again. Along with all of the other requirements. So the way this works is you get the conversation into the method, along with a list of requirements. You then? For each requirement. 
Uh, validate or call the validate function on the requirement with 
It looking at, Or at least the large language model version of the requirement Checker. Would look at the last message. 
Caching. With a bookmark. 
Uh, at the current Um, Perspective. Message or the last message in the conversation? And, The message before that, that's where it would set a bookmark and it would add the requirements text along with a description of, you know, like what its goal is. It's you're asking it an automated question. Uh, did you follow this requirement? And here's the descriptions. Uh just or here's the requirements description. And then it's expected to return in a certain format. And you get an answer back and you parse it. And if it's passable, then Um, 
You move forward. And this is all happening with inside that requirements or that llm. Requirements. Uh, 
Verification function. 
And it returns, it ultimately returns true that it's past. 
You iterate through all of the requirements and if it passes, you simply return. 
Uh, 
Original uh, list of messages. 
That were passed to you. And if 
If any of them don't pass. 
Then your continuously modifying that list of messages. Uh, taking you every time one of them doesn't pass you. Uh, 
Then asks, the requirement. Uh, to Redraft the perspective message, you replace the perspective message with whatever comes out as a result. And 
You iterate the requirements again at that point. So if it fails You're you're gonna run all of the requirements a second time. And the reason for that is 
Um, 
You know, if you If you went through, let's say,
Um, One requirement is. Don't apologize to me. The next requirement is indent your code with tabs instead of spaces. Then. Uh, if In the act of having completed. The first requirement, it removes the apology. Uh, replaces the perspective message with the non-apologetic answer. And then you evaluate the Uh, requirement Checker on that again, you ask did you actually remove the apology? Or, you know, did you did you remove the apology now with different examples? Um, Then. 
When you? 
Ask. 
Uh, did you indent your code with spaces? Or tabs instead of spaces. 
Uh, Then it comes back and says, no, I didn't. Uh, you tell it to replace that requirement or replace the perspective message the the redraft, I guess, whatever, the latest perspective, Message version you're on, you ask it to replace the tabs or the spaces with tabs. 
You can end up having to run the first requirement again. Because now, maybe it apologizes again.

And then, with the way thats writen, you would be able to have 1 set of requirements like for any time an LLM responds to a person in a chat window, another set of requirements for any time an LLM uses the terminal, another for any time it writes code, another for a given project, or for a given file that it might edit... Like, you could leave code requirements as well, something that is applied only when a LLM tasked with modifying code attempts to modify a specific file. And generally, except for how that list of requirements is built out, the process for validating a response against some set of requirements would basically be the same.
